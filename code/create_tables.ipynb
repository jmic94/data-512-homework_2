{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c0d3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json, time, urllib.parse\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aaa3e6",
   "metadata": {},
   "source": [
    "### Set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c437ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "WD = '/Users/johnmichael/Documents/DATA512/data-512-homework_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f376177e",
   "metadata": {},
   "source": [
    "### Import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a84baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities = pd.read_csv(os.path.join(WD, 'input/us_cities_by_state_SEPT.2023.csv'))\n",
    "us_pop = pd.read_excel(os.path.join(WD, 'input/NST-EST2022-POP.xlsx'), skiprows=3)\n",
    "us_regions = pd.read_excel(os.path.join(WD, 'input/US States by Region - US Census Bureau.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1086e531",
   "metadata": {},
   "source": [
    "### Clean input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ad8c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Clean regions file so that each row has a\n",
    "unique region, division and state combination.\n",
    "'''\n",
    "us_regions_clean = us_regions.ffill()\n",
    "us_regions_clean.dropna(subset='STATE', inplace=True)\n",
    "us_regions_clean.drop_duplicates(subset='STATE', ignore_index=True, inplace=True)\n",
    "us_regions_clean.columns = us_regions_clean.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10e739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Clean US population data so that each row\n",
    "contains a state and the 2022 population.\n",
    "'''\n",
    "us_pop_clean = us_pop.copy()\n",
    "us_pop_clean.columns = ['state', 'pop_2020_est', 'pop_2020', 'pop_2021', 'pop_2022']\n",
    "us_pop_clean = us_pop_clean[us_pop_clean.state.str.contains('^\\.', na=False)]\n",
    "us_pop_clean['state'] = us_pop_clean.state.str.slice(1)\n",
    "us_pop_clean = us_pop_clean[['state', 'pop_2022']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40821d",
   "metadata": {},
   "source": [
    "### Request page info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65e8060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows\n",
    "us_cities.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "812483b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Any API-related code below are adopted from wp_page_info_example.ipynb\n",
    "and wp_ores_liftwing_example.ipynb. Both notebooks are in the repository\n",
    "https://github.com/jmic94/data-512-homework_2.\n",
    "See the notebooks for full CC-BY license information.\n",
    "'''\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = 'https://en.wikipedia.org/w/api.php'\n",
    "\n",
    "'''\n",
    "We'll assume that there needs to be some throttling for these requests - \n",
    "we should always be nice to a free data resource\n",
    "'''\n",
    "API_LATENCY_ASSUMED = 0.002 # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (20.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "'''\n",
    "When making automated requests we should include something that is unique\n",
    "to the person making the request. This should include an email - your UW email\n",
    "would be good to put in there.\n",
    "'''\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<jmic94@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles we are requesting info for\n",
    "ARTICLE_TITLES = list(us_cities.page_title)\n",
    "\n",
    "'''\n",
    "This is a string of additional page properties that can be returned.\n",
    "See the Info documentation for what can be included.\n",
    "If you don't want any this can simply be the empty string.\n",
    "'''\n",
    "PAGEINFO_EXTENDED_PROPERTIES = 'talkid|url|watched|watchers'\n",
    "# PAGEINFO_EXTENDED_PROPERTIES = ''\n",
    "\n",
    "# This template lists the basic parameters for making this request\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    'action': 'query',\n",
    "    'format': 'json',\n",
    "    'titles': '', # to simplify this should be a single page title at a time\n",
    "    'prop': 'info',\n",
    "    'inprop': PAGEINFO_EXTENDED_PROPERTIES\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5329311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception('Must supply an article title to make a pageinfo request.')\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da0e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "page_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0b105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make page info requests\n",
    "# Create log file to track progress\n",
    "log_file_name = os.path.join(WD, 'intermediate/page_info_requests.log')\n",
    "if os.path.exists(log_file_name):\n",
    "    with open(log_file_name, 'w'):\n",
    "        pass\n",
    "logger.remove()\n",
    "logger.add(log_file_name,\n",
    "           format='{time:YYYY-MM-DD HH:mm:ss} | {message}',\n",
    "           level='INFO',\n",
    "           backtrace=False,\n",
    "           diagnose=False)\n",
    "\n",
    "for i, page in enumerate(ARTICLE_TITLES):\n",
    "    logger.info(f'Progress: {i} - {page}')\n",
    "    info = request_pageinfo_per_article(page)\n",
    "    df = pd.DataFrame(info['query']['pages'].values())\n",
    "    page_df = pd.concat([page_df, df])\n",
    "logger.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86cbd8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 17116.7s\n"
     ]
    }
   ],
   "source": [
    "# Export page info request as csv\n",
    "page_df.to_csv(os.path.join(WD, 'intermediate/page_info.csv'), index=False)\n",
    "print(f'Elapsed time: {time.time() - start:.1f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26fceccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_df = pd.read_csv(os.path.join(WD, 'intermediate/page_info.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c3d53",
   "metadata": {},
   "source": [
    "### Request Article Quality Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37977104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access token\n",
    "'''\n",
    "Store the access token in a .env file in a local clone of the repository.\n",
    "This .env will be ignored by the repository and will not be pushed to the\n",
    "remote Git.\n",
    "'''\n",
    "load_dotenv()\n",
    "ACCESS_TOKEN = os.getenv('access_token')\n",
    "\n",
    "# The current LiftWing ORES API endpoint and prediction model\n",
    "API_ORES_LIFTWING_ENDPOINT = 'https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict'\n",
    "API_ORES_EN_QUALITY_MODEL = 'enwiki-articlequality'\n",
    "\n",
    "'''\n",
    "The throttling rate is a function of the Access token that you are granted\n",
    "when you request the token. The constants come from dissecting the token\n",
    "and getting the rate limits from the granted token. An example of that is below.\n",
    "'''\n",
    "API_LATENCY_ASSUMED = 0.002 # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (60.0/5000.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "'''\n",
    "When making automated requests we should include something that is\n",
    "unique to the person making the request. This should include an email\n",
    "- your UW email would be good to put in there. Because all LiftWing\n",
    "API requests require some form of authentication, you need to provide\n",
    "your access token as part of the header too.\n",
    "'''\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': '<jmic94@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {ACCESS_TOKEN}'\n",
    "}\n",
    "\n",
    "# This is a template for the parameters that we need to supply in the headers of an API request\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address': '', # your email address should go here\n",
    "    'access_token': '' # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "'''\n",
    "A dictionary of English Wikipedia article titles (keys) and\n",
    "sample revision IDs that can be used for this ORES scoring example.\n",
    "'''\n",
    "# ARTICLE_REVISIONS = {'Bison': 1085687913,\n",
    "#                      'Northern flicker': 1086582504,\n",
    "#                      'Red squirrel': 1083787665,\n",
    "#                      'Chinook salmon': 1085406228,\n",
    "#                      'Horseshoe bat': 1060601936}\n",
    "# ARTICLE_REVISIONS = page_df[['title', 'lastrevid']].set_index('title')['lastrevid'].to_dict()\n",
    "\n",
    "\n",
    "# This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    'lang': 'en', # required that its english - we're scoring English Wikipedia revisions\n",
    "    'rev_id': '', # this request requires a revision id\n",
    "    'features': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73c620ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_ores_score_per_article(article_revid = None,\n",
    "                                   email_address=None,\n",
    "                                   access_token=None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT, \n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL, \n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE, \n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE, \n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    \n",
    "    # Make sure we have an article revision id, email and token\n",
    "    # This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "    \n",
    "    # Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception('Must provide an article revision id (rev_id) to score articles')\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "    \n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "    \n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        #response = requests.get(request_url, headers=headers)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8145bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "score_df = pd.DataFrame()\n",
    "hparams = REQUEST_HEADER_PARAMS_TEMPLATE.copy()\n",
    "hparams['email_address'] = \"jmic94@uw.edu\"\n",
    "hparams['access_token'] = ACCESS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cd74c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log file to track progress\n",
    "log_file_name = os.path.join(WD, 'intermediate/ores_requests.log')\n",
    "if os.path.exists(log_file_name):\n",
    "    with open(log_file_name, 'w'):\n",
    "        pass\n",
    "logger.remove()\n",
    "logger.add(log_file_name,\n",
    "           format='{time:YYYY-MM-DD HH:mm:ss} | {message}',\n",
    "           level='INFO',\n",
    "           backtrace=False,\n",
    "           diagnose=False)\n",
    "\n",
    "for i in range(page_df.shape[0]):\n",
    "    title = page_df.title[i]\n",
    "    rev_id = int(page_df.lastrevid[i])\n",
    "    logger.info(f'Progress: {i} - {title}')\n",
    "    rd = ORES_REQUEST_DATA_TEMPLATE.copy()\n",
    "    rd['rev_id'] = rev_id\n",
    "    score = request_ores_score_per_article(request_data=rd,\n",
    "                                           header_params=hparams)\n",
    "    quality = score['enwiki']['scores'][str(rev_id)][\n",
    "        'articlequality']['score']['prediction']\n",
    "    df = pd.DataFrame({'title': [title],\n",
    "                       'rev_id': [rev_id],\n",
    "                       'quality': [quality]})\n",
    "    score_df = pd.concat([score_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "37fb47dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 52723.2s\n"
     ]
    }
   ],
   "source": [
    "score_df.to_csv(os.path.join(WD, 'intermediate/scores.csv'), index=False)\n",
    "logger.remove()\n",
    "print(f'Elapsed time: {time.time() - start:.1f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43362f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.read_csv(os.path.join(WD, 'intermediate/scores.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e698fa26",
   "metadata": {},
   "source": [
    "### Combine data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4feccc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = page_df.copy()\n",
    "\n",
    "# Get state column\n",
    "combined_df = combined_df.merge(us_cities, how='left',\n",
    "                                left_on='title', right_on='page_title')\n",
    "\n",
    "# Clean state names\n",
    "combined_df['state'] = np.where(combined_df['state'] == 'Georgia_(U.S._state)',\n",
    "                                'Georgia', combined_df['state'])\n",
    "combined_df['state'] = combined_df.state.str.replace('_', ' ')\n",
    "\n",
    "# Merge with scores data\n",
    "combined_df = combined_df.merge(score_df[['rev_id', 'quality']],\n",
    "                                left_on='lastrevid',\n",
    "                                right_on='rev_id',\n",
    "                                how='left')\n",
    "\n",
    "# Merge with regional division data\n",
    "combined_df = combined_df.merge(us_regions_clean, how='left', on='state')\n",
    "\n",
    "# Merge with population data\n",
    "combined_df = combined_df.merge(us_pop_clean, how='left', on='state')\n",
    "\n",
    "# Rename columns\n",
    "combined_df.rename(columns={'title': 'article_title',\n",
    "                            'lastrevid': 'revision_id',\n",
    "                            'quality': 'article_quality',\n",
    "                            'division': 'regional_division',\n",
    "                            'pop_2022': 'population'},\n",
    "                   inplace=True)\n",
    "combined_df = combined_df[['state', 'regional_division', 'population',\n",
    "                           'article_title', 'revision_id', 'article_quality']]\n",
    "combined_df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "# Export combined data set\n",
    "combined_df.to_csv(os.path.join(WD, 'output/wp_scored_city_articles_by_state.csv'),\n",
    "                   index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c73b8b0",
   "metadata": {},
   "source": [
    "### Create analysis tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa8f318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_per_pop = combined_df.groupby('state')['article_title'].count().reset_index()\n",
    "state_per_pop.rename(columns={'article_title': 'n_articles'}, inplace=True)\n",
    "state_per_pop = state_per_pop.merge(combined_df[['state', 'population']],\n",
    "                                    how='left', on='state')\n",
    "state_per_pop.drop_duplicates(inplace=True, ignore_index=True)\n",
    "state_per_pop['articles_per_pop'] = state_per_pop.n_articles / state_per_pop.population\n",
    "state_per_pop.sort_values(by='articles_per_pop', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7eea46",
   "metadata": {},
   "source": [
    "#### Top 10 US States by Article Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e8cee51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>n_articles</th>\n",
       "      <th>population</th>\n",
       "      <th>articles_per_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>329</td>\n",
       "      <td>647064.0</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>356</td>\n",
       "      <td>779261.0</td>\n",
       "      <td>0.000457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Maine</td>\n",
       "      <td>483</td>\n",
       "      <td>1385340.0</td>\n",
       "      <td>0.000349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>311</td>\n",
       "      <td>909824.0</td>\n",
       "      <td>0.000342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>1043</td>\n",
       "      <td>3200517.0</td>\n",
       "      <td>0.000326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>149</td>\n",
       "      <td>733583.0</td>\n",
       "      <td>0.000203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>2556</td>\n",
       "      <td>12972008.0</td>\n",
       "      <td>0.000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>1773</td>\n",
       "      <td>10034113.0</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>99</td>\n",
       "      <td>581381.0</td>\n",
       "      <td>0.000170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>234</td>\n",
       "      <td>1395231.0</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           state  n_articles  population  articles_per_pop\n",
       "0        Vermont         329    647064.0          0.000508\n",
       "1   North Dakota         356    779261.0          0.000457\n",
       "2          Maine         483   1385340.0          0.000349\n",
       "3   South Dakota         311    909824.0          0.000342\n",
       "4           Iowa        1043   3200517.0          0.000326\n",
       "5         Alaska         149    733583.0          0.000203\n",
       "6   Pennsylvania        2556  12972008.0          0.000197\n",
       "7       Michigan        1773  10034113.0          0.000177\n",
       "8        Wyoming          99    581381.0          0.000170\n",
       "9  New Hampshire         234   1395231.0          0.000168"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_per_pop.head(10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597f7c2",
   "metadata": {},
   "source": [
    "#### Bottom 10 US States by Article Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78fba169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>n_articles</th>\n",
       "      <th>population</th>\n",
       "      <th>articles_per_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>50</td>\n",
       "      <td>10698973.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>19</td>\n",
       "      <td>3177772.0</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>California</td>\n",
       "      <td>482</td>\n",
       "      <td>39029342.0</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>91</td>\n",
       "      <td>7359197.0</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>133</td>\n",
       "      <td>8683619.0</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Florida</td>\n",
       "      <td>412</td>\n",
       "      <td>22244823.0</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>75</td>\n",
       "      <td>4019800.0</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>63</td>\n",
       "      <td>2937150.0</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>157</td>\n",
       "      <td>6164660.0</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>192</td>\n",
       "      <td>5892539.0</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            state  n_articles  population  articles_per_pop\n",
       "0  North Carolina          50  10698973.0          0.000005\n",
       "1          Nevada          19   3177772.0          0.000006\n",
       "2      California         482  39029342.0          0.000012\n",
       "3         Arizona          91   7359197.0          0.000012\n",
       "4        Virginia         133   8683619.0          0.000015\n",
       "5         Florida         412  22244823.0          0.000019\n",
       "6        Oklahoma          75   4019800.0          0.000019\n",
       "7          Kansas          63   2937150.0          0.000021\n",
       "8        Maryland         157   6164660.0          0.000025\n",
       "9       Wisconsin         192   5892539.0          0.000033"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_per_pop.tail(10).sort_values(by='articles_per_pop').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33c1b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_per_pop_hq = combined_df.copy()\n",
    "state_per_pop_hq['hq'] = np.where(state_per_pop_hq['article_quality'].isin(['FA', 'GA']), 1, 0)\n",
    "state_per_pop_hq = state_per_pop_hq.groupby('state')['hq'].sum().reset_index()\n",
    "state_per_pop_hq.rename(columns={'hq': 'n_hq_articles'}, inplace=True)\n",
    "state_per_pop_hq = state_per_pop_hq.merge(combined_df[['state', 'population']],\n",
    "                                    how='left', on='state')\n",
    "state_per_pop_hq.drop_duplicates(inplace=True, ignore_index=True)\n",
    "state_per_pop_hq['hq_articles_per_pop'] = state_per_pop_hq.n_hq_articles / state_per_pop_hq.population\n",
    "state_per_pop_hq.sort_values(by='hq_articles_per_pop', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af053d36",
   "metadata": {},
   "source": [
    "#### Top 10 US States by High Quality Article Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60100509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>n_hq_articles</th>\n",
       "      <th>population</th>\n",
       "      <th>hq_articles_per_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>45</td>\n",
       "      <td>647064.0</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>39</td>\n",
       "      <td>581381.0</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>56</td>\n",
       "      <td>909824.0</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>106</td>\n",
       "      <td>1775156.0</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Montana</td>\n",
       "      <td>55</td>\n",
       "      <td>1122867.0</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>63</td>\n",
       "      <td>1395231.0</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>566</td>\n",
       "      <td>12972008.0</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>263</td>\n",
       "      <td>6177957.0</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>31</td>\n",
       "      <td>733583.0</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>379</td>\n",
       "      <td>9261699.0</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            state  n_hq_articles  population  hq_articles_per_pop\n",
       "42        Vermont             45    647064.0             0.000070\n",
       "47        Wyoming             39    581381.0             0.000067\n",
       "38   South Dakota             56    909824.0             0.000062\n",
       "45  West Virginia            106   1775156.0             0.000060\n",
       "24        Montana             55   1122867.0             0.000049\n",
       "26  New Hampshire             63   1395231.0             0.000045\n",
       "35   Pennsylvania            566  12972008.0             0.000044\n",
       "23       Missouri            263   6177957.0             0.000043\n",
       "1          Alaska             31    733583.0             0.000042\n",
       "27     New Jersey            379   9261699.0             0.000041"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hq = High Quality\n",
    "state_per_pop_hq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcbdec7",
   "metadata": {},
   "source": [
    "#### Bottom 10 US States by High Quality Article Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13c022e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>n_hq_articles</th>\n",
       "      <th>population</th>\n",
       "      <th>hq_articles_per_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>20</td>\n",
       "      <td>10698973.0</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>18</td>\n",
       "      <td>8683619.0</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>8</td>\n",
       "      <td>3177772.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>24</td>\n",
       "      <td>7359197.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>172</td>\n",
       "      <td>39029342.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Florida</td>\n",
       "      <td>119</td>\n",
       "      <td>22244823.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>New York</td>\n",
       "      <td>111</td>\n",
       "      <td>19677151.0</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>42</td>\n",
       "      <td>6164660.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>22</td>\n",
       "      <td>2937150.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>31</td>\n",
       "      <td>4019800.0</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            state  n_hq_articles  population  hq_articles_per_pop\n",
       "0  North Carolina             20  10698973.0             0.000002\n",
       "1        Virginia             18   8683619.0             0.000002\n",
       "2          Nevada              8   3177772.0             0.000003\n",
       "3         Arizona             24   7359197.0             0.000003\n",
       "4      California            172  39029342.0             0.000004\n",
       "5         Florida            119  22244823.0             0.000005\n",
       "6        New York            111  19677151.0             0.000006\n",
       "7        Maryland             42   6164660.0             0.000007\n",
       "8          Kansas             22   2937150.0             0.000007\n",
       "9        Oklahoma             31   4019800.0             0.000008"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_per_pop_hq.tail(10).sort_values(by='hq_articles_per_pop').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "170c5e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "div_per_pop = combined_df.groupby('regional_division')['article_title'].count().reset_index()\n",
    "div_pops = combined_df[['state', 'regional_division', 'population']].drop_duplicates()\n",
    "div_pops = div_pops.groupby('regional_division')['population'].sum().reset_index()\n",
    "div_per_pop.rename(columns={'article_title': 'n_articles'}, inplace=True)\n",
    "div_per_pop = div_per_pop.merge(div_pops, how='left', on='regional_division')\n",
    "div_per_pop.drop_duplicates(inplace=True, ignore_index=True)\n",
    "div_per_pop['articles_per_pop'] = div_per_pop.n_articles / div_per_pop.population\n",
    "div_per_pop.sort_values(by='articles_per_pop', ascending=False, inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c06435",
   "metadata": {},
   "source": [
    "#### US Regional Divisions by Article Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb9aecb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional_division</th>\n",
       "      <th>n_articles</th>\n",
       "      <th>population</th>\n",
       "      <th>articles_per_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>3578</td>\n",
       "      <td>19721893.0</td>\n",
       "      <td>0.000181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New England</td>\n",
       "      <td>1437</td>\n",
       "      <td>11503343.0</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>4754</td>\n",
       "      <td>47097779.0</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>3781</td>\n",
       "      <td>41910858.0</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>1529</td>\n",
       "      <td>19578002.0</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>2103</td>\n",
       "      <td>41685250.0</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>1189</td>\n",
       "      <td>25514320.0</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>1850</td>\n",
       "      <td>66781137.0</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>1304</td>\n",
       "      <td>53229044.0</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regional_division  n_articles  population  articles_per_pop\n",
       "0  West North Central        3578  19721893.0          0.000181\n",
       "1         New England        1437  11503343.0          0.000125\n",
       "2  East North Central        4754  47097779.0          0.000101\n",
       "3     Middle Atlantic        3781  41910858.0          0.000090\n",
       "4  East South Central        1529  19578002.0          0.000078\n",
       "5  West South Central        2103  41685250.0          0.000050\n",
       "6            Mountain        1189  25514320.0          0.000047\n",
       "7      South Atlantic        1850  66781137.0          0.000028\n",
       "8             Pacific        1304  53229044.0          0.000024"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_per_pop.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e91e2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "div_per_pop_hq = combined_df.copy()\n",
    "div_per_pop_hq['hq'] = np.where(div_per_pop_hq['article_quality'].isin(['FA', 'GA']), 1, 0)\n",
    "div_per_pop_hq = div_per_pop_hq.groupby('regional_division')['hq'].sum().reset_index()\n",
    "div_per_pop_hq.rename(columns={'hq': 'n_hq_articles'}, inplace=True)\n",
    "div_per_pop_hq = div_per_pop_hq.merge(div_pops[['regional_division', 'population']],\n",
    "                                how='left', on='regional_division')\n",
    "div_per_pop_hq.drop_duplicates(inplace=True, ignore_index=True)\n",
    "div_per_pop_hq['hq_articles_per_pop'] = div_per_pop_hq.n_hq_articles / div_per_pop_hq.population\n",
    "div_per_pop_hq.sort_values(by='hq_articles_per_pop', ascending=False, inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f457eb17",
   "metadata": {},
   "source": [
    "#### US Regional Divisions by High Quality Article Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56ee825b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional_division</th>\n",
       "      <th>n_hq_articles</th>\n",
       "      <th>population</th>\n",
       "      <th>hq_articles_per_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>640</td>\n",
       "      <td>19721893.0</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>1056</td>\n",
       "      <td>41910858.0</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New England</td>\n",
       "      <td>225</td>\n",
       "      <td>11503343.0</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>317</td>\n",
       "      <td>19578002.0</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>717</td>\n",
       "      <td>47097779.0</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>634</td>\n",
       "      <td>41685250.0</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>336</td>\n",
       "      <td>25514320.0</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>489</td>\n",
       "      <td>53229044.0</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>526</td>\n",
       "      <td>66781137.0</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regional_division  n_hq_articles  population  hq_articles_per_pop\n",
       "0  West North Central            640  19721893.0             0.000032\n",
       "1     Middle Atlantic           1056  41910858.0             0.000025\n",
       "2         New England            225  11503343.0             0.000020\n",
       "3  East South Central            317  19578002.0             0.000016\n",
       "4  East North Central            717  47097779.0             0.000015\n",
       "5  West South Central            634  41685250.0             0.000015\n",
       "6            Mountain            336  25514320.0             0.000013\n",
       "7             Pacific            489  53229044.0             0.000009\n",
       "8      South Atlantic            526  66781137.0             0.000008"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_per_pop_hq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
